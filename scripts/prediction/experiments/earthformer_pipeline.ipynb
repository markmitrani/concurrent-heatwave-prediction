{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9f8d68",
   "metadata": {},
   "source": [
    "# 1. Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfddcc",
   "metadata": {},
   "source": [
    "This notebook trains a classifier to recognize spatiotemporal weather archetypes using Earthformer, a pretrained geospatial transformer. The pipeline includes data preprocessing, model adaptation, transfer learning, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b0930",
   "metadata": {},
   "source": [
    "# 2. Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a8909cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered based on first usage in notebook\n",
    "import numpy as np\n",
    "import h5py\n",
    "import xarray as xr\n",
    "import torch.nn.functional as F\n",
    "from datetime import timedelta\n",
    "import torch\n",
    "import os\n",
    "#from earthformer.cuboid_transformer.cuboid_transformer import CuboidTransformerModel\n",
    "#from earthformer.utils.utils import download\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a69e2a",
   "metadata": {},
   "source": [
    "# 3. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fee29",
   "metadata": {},
   "source": [
    "## 3.1 Load raw input data & interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c950317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<xarray.Dataset>\n",
      "Dimensions:    (time: 9200, lon: 170, lat: 29, plev: 1, bnds: 2)\n",
      "Coordinates:\n",
      "  * time       (time) datetime64[ns] 2000-06-01T12:00:00 ... 2009-08-31T12:00:00\n",
      "  * lon        (lon) float64 -179.3 -177.2 -175.1 -173.0 ... 173.0 175.1 177.2\n",
      "  * lat        (lat) float64 15.79 17.89 20.0 22.11 ... 68.42 70.53 72.63 74.74\n",
      "  * plev       (plev) float64 2.5e+04\n",
      "    dayofyear  (time) int64 ...\n",
      "Dimensions without coordinates: bnds\n",
      "Data variables:\n",
      "    lon_bnds   (time, lon, bnds) float64 ...\n",
      "    lat_bnds   (time, lat, bnds) float64 ...\n",
      "    stream     (time, plev, lat, lon) float64 ...\n"
     ]
    }
   ],
   "source": [
    "# import both nc's\n",
    "stream_path = \"../../../data/deseason_smsub/lentis_stream250_JJA_2deg_101_deseason_spatialsub.nc\"\n",
    "#stream_path = \"../../../data/deseason_smsub_sqrtcosw/lentis_stream250_JJA_2deg_101_deseason_smsub_sqrtcosw.nc\"\n",
    "dataset_stream = xr.open_dataset(stream_path)\n",
    "\n",
    "#print(dataset_stream)\n",
    "\n",
    "tas_path = \"../../../data/deseason_smsub/lentis_tas_JJA_2deg_101_deseason.nc\"\n",
    "dataset_tas = xr.open_dataset(tas_path)\n",
    "\n",
    "# get S_PCHA from archetypes file\n",
    "with h5py.File('../../../data/deseason_smsub/pcha_results_8a.hdf5', 'r') as f: # run from mmi393 directory or gives error\n",
    "        S_PCHA = f['/S_PCHA'][:]\n",
    "\n",
    "# group indices based on whichever archetype is maximum there\n",
    "arch_indices = np.argmax(S_PCHA, axis=0)\n",
    "\n",
    "# join the nc's together\n",
    "dataset_comb = dataset_stream.assign(tas=dataset_tas['tas'])\n",
    "\n",
    "# add labels from archetypes into the dataset\n",
    "arch_da = xr.DataArray(arch_indices, dims=\"time\", coords={\"time\": dataset_comb.time})\n",
    "dataset_comb_labeled = dataset_comb.assign(archetype=arch_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dbc60",
   "metadata": {},
   "source": [
    "Load xarray data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f9e2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = dataset_comb['stream'].squeeze('plev').values  # (T, lat, lon)\n",
    "tas = dataset_comb['tas'].values                        # (T, lat, lon)\n",
    "\n",
    "# Extract and squeeze stream function\n",
    "stream = dataset_comb['stream'].squeeze('plev').values  # (T, H, W)\n",
    "tas = dataset_comb['tas'].values                        # (T, H, W)\n",
    "\n",
    "# Stack the variables along the channel axis\n",
    "x_np = np.stack([stream, tas], axis=-1)  # shape: (T, H, W, C) where C = 2\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "x_tensor = torch.from_numpy(x_np).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d630dcd",
   "metadata": {},
   "source": [
    "Interpolate from original (H, W) shape to (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea62e410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final resized shape: torch.Size([9200, 128, 128, 2])\n"
     ]
    }
   ],
   "source": [
    "# Change shape from (T, H, W, C) â†’ (T, C, H, W) for interp\n",
    "x_tensor_perm = x_tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "# Resize spatial dimensions to 128x128\n",
    "x_tensor_resized = F.interpolate(x_tensor_perm, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "# Restore dimension ordering\n",
    "x_tensor_resized = x_tensor_resized.permute(0, 2, 3, 1)\n",
    "\n",
    "# Check final shape\n",
    "print(\"Final resized shape:\", x_tensor_resized.shape) # (T, 128, 128, C)\n",
    "\n",
    "x_tensor = x_tensor_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb756e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_tensor(x_tensor, target_size=(128, 128)):\n",
    "    x_tensor_perm = x_tensor.permute(0, 3, 1, 2)\n",
    "    x_tensor_resized = F.interpolate(x_tensor_perm, size=target_size, mode='bilinear', align_corners=False)\n",
    "    x_tensor_resized = x_tensor_resized.permute(0, 2, 3, 1)\n",
    "    return x_tensor_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbd07d",
   "metadata": {},
   "source": [
    "## 3.2 Construct target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888b143e",
   "metadata": {},
   "source": [
    "NB: Some of the data will be cut during target construction due to the lead time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341733fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 128, 128, 2)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_list[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Stack into tensors\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m x_final \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_list\u001b[49m\u001b[43m)\u001b[49m)              \u001b[38;5;66;03m# shape: (N, H, W, C)\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# TODO change y into one-hot vector encoding?\u001b[39;00m\n\u001b[1;32m     22\u001b[0m y_final \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_list, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)  \u001b[38;5;66;03m# shape: (N,)\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/netcdf_env_backup/lib/python3.9/warnings.py:403\u001b[0m, in \u001b[0;36mWarningMessage.__init__\u001b[0;34m(self, message, category, filename, lineno, file, line, source)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mWarningMessage\u001b[39;00m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[1;32m    400\u001b[0m     _WARNING_DETAILS \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    401\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 403\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, message, category, filename, lineno, file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    404\u001b[0m                  line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, source\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m message\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcategory \u001b[38;5;241m=\u001b[39m category\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "l = 7  # lead time\n",
    "time = dataset_comb['time'].values  # format: datetime64\n",
    "arch_labels = arch_da.values        # (9200,)\n",
    "\n",
    "x_all = x_tensor  # shape: (T, H, W, C)\n",
    "x_list = []\n",
    "y_list = []\n",
    "kept_time_indices = []\n",
    "\n",
    "# Makes it so that examples from different years do not get combined\n",
    "# TODO Add data from September to include last week of August?\n",
    "for t in range(len(time) - l):\n",
    "    target_time = time[t] + np.timedelta64(l, 'D')\n",
    "    if time[t + l] == target_time:\n",
    "        x_list.append(np.expand_dims(x_all[t], 0))\n",
    "        y_list.append(np.expand_dims(arch_labels[t + l], 0))\n",
    "        kept_time_indices.append([t])\n",
    "print(x_list[0].shape)\n",
    "# Stack into tensors\n",
    "x_final = torch.stack(torch.tensor(x_list))              # shape: (N, H, W, C)\n",
    "# TODO change y into one-hot vector encoding?\n",
    "y_final = torch.tensor(y_list, dtype=torch.long)  # shape: (N,)\n",
    "\n",
    "print(f\"x_final shape: {x_final.shape}\") # approx. 8% of the dataset is cut\n",
    "print(f\"y_final shape: {y_final.shape}\")\n",
    "print(kept_time_indices[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9a9a5984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1, 128, 128, 2])\n"
     ]
    }
   ],
   "source": [
    "lead_time = l\n",
    "input_seq_len = 10\n",
    "arch_indices = np.argmax(S_PCHA, axis=0)\n",
    "arch_da = xr.DataArray(arch_indices, dims=\"time\", coords={\"time\": dataset_comb.time})\n",
    "arch_labels = arch_da.values\n",
    "\n",
    "stream = dataset_comb['stream'].squeeze('plev').values\n",
    "tas = dataset_comb['tas'].values\n",
    "x_np = np.stack([stream, tas], axis=-1)\n",
    "x_tensor = torch.from_numpy(x_np).float()\n",
    "\n",
    "x_tensor = interpolate_tensor(x_tensor)\n",
    "\n",
    "time = dataset_comb['time'].values\n",
    "x_list, y_list, kept_time_indices = [], [], []\n",
    "\n",
    "for t in range(len(time) - lead_time):\n",
    "    target_time = time[t] + np.timedelta64(lead_time, 'D')\n",
    "    if time[t + lead_time] == target_time:\n",
    "        #pad = torch.zeros((input_seq_len - 1, *x_tensor.shape[1:]))  # (9, H, W, C)\n",
    "        #x_padded = torch.cat([pad, x_tensor[t].unsqueeze(0)], dim=0)  # (10, H, W, C)\n",
    "        x_list.append(x_tensor[t].unsqueeze(0))\n",
    "        y_list.append(arch_labels[t + lead_time])\n",
    "        kept_time_indices.append(t)\n",
    "\n",
    "x_final = torch.stack(x_list)                       # shape: (N, 1, H, W, C)\n",
    "y_final = torch.tensor(y_list, dtype=torch.long)\n",
    "\n",
    "print(x_final[0:32].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8050d",
   "metadata": {},
   "source": [
    "## 3.3 Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67814e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "split = 0.8\n",
    "data_length = x_final.shape[0]\n",
    "x_train, x_test = x_final[:floor(data_length*split)], x_final[floor(data_length*split):]\n",
    "y_train, y_test = y_final[:floor(data_length*split)], y_final[floor(data_length*split):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff1091",
   "metadata": {},
   "source": [
    "# 4. Earthformer Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b04a2",
   "metadata": {},
   "source": [
    "## 4.1 Load Earthformer model config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aca583",
   "metadata": {},
   "source": [
    "Load state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c45c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"../../../data/pretrained\"\n",
    "\n",
    "pretrained_checkpoint_url = \"https://earthformer.s3.amazonaws.com/pretrained_checkpoints/earthformer_earthnet2021.pt\"\n",
    "local_checkpoint_path = os.path.join(save_dir, \"earthformer_earthnet2021.pt\")\n",
    "download(url=pretrained_checkpoint_url, path=local_checkpoint_path)\n",
    "\n",
    "state_dict = torch.load(local_checkpoint_path, map_location=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67744792",
   "metadata": {},
   "source": [
    "Load EarthNet2021 config, sourced from [Earthformer repository](https://github.com/amazon-science/earth-forecasting-transformer/blob/main/scripts/cuboid_transformer/earthnet_w_meso/earthformer_earthnet_v1.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "461c7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthformer_config = {\n",
    "    \"base_units\": 256,\n",
    "    \"block_units\": None,\n",
    "    \"scale_alpha\": 1.0,\n",
    "\n",
    "    \"enc_depth\": [1, 1],\n",
    "    \"dec_depth\": [1, 1],\n",
    "    \"enc_use_inter_ffn\": True,\n",
    "    \"dec_use_inter_ffn\": True,\n",
    "    \"dec_hierarchical_pos_embed\": False,\n",
    "\n",
    "    \"downsample\": 2,\n",
    "    \"downsample_type\": \"patch_merge\",\n",
    "    \"upsample_type\": \"upsample\",\n",
    "\n",
    "    \"num_global_vectors\": 2,\n",
    "    \"use_dec_self_global\": False,\n",
    "    \"dec_self_update_global\": True,\n",
    "    \"use_dec_cross_global\": False,\n",
    "    \"use_global_vector_ffn\": False,\n",
    "    \"use_global_self_attn\": True,\n",
    "    \"separate_global_qkv\": True,\n",
    "    \"global_dim_ratio\": 1,\n",
    "\n",
    "    \"attn_drop\": 0.1,\n",
    "    \"proj_drop\": 0.1,\n",
    "    \"ffn_drop\": 0.1,\n",
    "    \"num_heads\": 4,\n",
    "\n",
    "    \"ffn_activation\": \"gelu\",\n",
    "    \"gated_ffn\": False,\n",
    "    \"norm_layer\": \"layer_norm\",\n",
    "    \"padding_type\": \"zeros\",\n",
    "    \"pos_embed_type\": \"t+hw\",\n",
    "    \"use_relative_pos\": True,\n",
    "    \"self_attn_use_final_proj\": True,\n",
    "\n",
    "    \"checkpoint_level\": 0,\n",
    "\n",
    "    \"initial_downsample_type\": \"stack_conv\",\n",
    "    \"initial_downsample_activation\": \"leaky\",\n",
    "    \"initial_downsample_stack_conv_num_layers\": 2,\n",
    "    \"initial_downsample_stack_conv_dim_list\": [64, 256],\n",
    "    \"initial_downsample_stack_conv_downscale_list\": [2, 2],\n",
    "    \"initial_downsample_stack_conv_num_conv_list\": [2, 2],\n",
    "\n",
    "    \"attn_linear_init_mode\": \"0\",\n",
    "    \"ffn_linear_init_mode\": \"0\",\n",
    "    \"conv_init_mode\": \"0\",\n",
    "    \"down_up_linear_init_mode\": \"0\",\n",
    "    \"norm_init_mode\": \"0\",\n",
    "\n",
    "    \"padding_type\": \"ignore\",\n",
    "    \"dec_cross_last_n_frames\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a39f8",
   "metadata": {},
   "source": [
    "## 4.2 Initialize model & load pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5a1ab",
   "metadata": {},
   "source": [
    "Initialize Earthformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "04d79d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "EFmodel = CuboidTransformerModel(input_shape=[1, 128, 128, 2],\n",
    "                               target_shape=[1, 128, 128, 64],\n",
    "                               **earthformer_config)\n",
    "# TODO check temporal resolution - 1, 5, 10 as input?\n",
    "# TODO try original shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92898d72",
   "metadata": {},
   "source": [
    "Filter and log matching pretrained weights from state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf0fd415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n",
      "431\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = EFmodel.state_dict()\n",
    "print(len(model_state_dict))\n",
    "print(len(state_dict))\n",
    "# Filter the keys that match in name AND shape\n",
    "compatible_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k in model_state_dict and model_state_dict[k].shape == v.shape:\n",
    "        compatible_state_dict[k] = v\n",
    "        #print(f\"Loading: {k} | with shape: {v.shape}\")\n",
    "    else:\n",
    "        val = model_state_dict.get(k, 'MISSING')\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            val = val.shape\n",
    "        #print(f\"Skipping: {k} | pretrained shape: {v.shape} vs model shape: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0494ed",
   "metadata": {},
   "source": [
    "Load compatible keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "33d6b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys:\n",
      "['init_global_vectors', 'initial_encoder.conv_block_list.0.0.weight', 'dec_final_proj.weight', 'dec_final_proj.bias', 'encoder.blocks.0.0.attn_l.0.relative_position_bias_table', 'encoder.blocks.0.0.attn_l.0.relative_position_index', 'encoder.blocks.0.0.attn_l.1.relative_position_bias_table', 'encoder.blocks.0.0.attn_l.1.relative_position_index', 'encoder.blocks.1.0.attn_l.0.relative_position_bias_table', 'encoder.blocks.1.0.attn_l.0.relative_position_index', 'encoder.blocks.1.0.attn_l.1.relative_position_bias_table', 'encoder.blocks.1.0.attn_l.1.relative_position_index', 'enc_pos_embed.T_embed.weight', 'z_proj.weight', 'z_proj.bias', 'dec_pos_embed.T_embed.weight', 'decoder.self_blocks.0.0.ffn_l.0.ffn_1.weight', 'decoder.self_blocks.0.0.ffn_l.0.ffn_1.bias', 'decoder.self_blocks.0.0.ffn_l.0.ffn_2.weight', 'decoder.self_blocks.0.0.ffn_l.0.ffn_2.bias', 'decoder.self_blocks.0.0.ffn_l.0.layer_norm.weight', 'decoder.self_blocks.0.0.ffn_l.0.layer_norm.bias', 'decoder.self_blocks.0.0.ffn_l.1.ffn_1.weight', 'decoder.self_blocks.0.0.ffn_l.1.ffn_1.bias', 'decoder.self_blocks.0.0.ffn_l.1.ffn_2.weight', 'decoder.self_blocks.0.0.ffn_l.1.ffn_2.bias', 'decoder.self_blocks.0.0.ffn_l.1.layer_norm.weight', 'decoder.self_blocks.0.0.ffn_l.1.layer_norm.bias', 'decoder.self_blocks.0.0.attn_l.0.relative_position_bias_table', 'decoder.self_blocks.0.0.attn_l.0.relative_position_index', 'decoder.self_blocks.0.0.attn_l.0.qkv.weight', 'decoder.self_blocks.0.0.attn_l.0.proj.weight', 'decoder.self_blocks.0.0.attn_l.0.proj.bias', 'decoder.self_blocks.0.0.attn_l.0.norm.weight', 'decoder.self_blocks.0.0.attn_l.0.norm.bias', 'decoder.self_blocks.0.0.attn_l.1.relative_position_bias_table', 'decoder.self_blocks.0.0.attn_l.1.relative_position_index', 'decoder.self_blocks.0.0.attn_l.1.qkv.weight', 'decoder.self_blocks.0.0.attn_l.1.proj.weight', 'decoder.self_blocks.0.0.attn_l.1.proj.bias', 'decoder.self_blocks.0.0.attn_l.1.norm.weight', 'decoder.self_blocks.0.0.attn_l.1.norm.bias', 'decoder.cross_blocks.0.0.ffn_l.0.ffn_1.weight', 'decoder.cross_blocks.0.0.ffn_l.0.ffn_1.bias', 'decoder.cross_blocks.0.0.ffn_l.0.ffn_2.weight', 'decoder.cross_blocks.0.0.ffn_l.0.ffn_2.bias', 'decoder.cross_blocks.0.0.ffn_l.0.layer_norm.weight', 'decoder.cross_blocks.0.0.ffn_l.0.layer_norm.bias', 'decoder.cross_blocks.0.0.ffn_l.1.ffn_1.weight', 'decoder.cross_blocks.0.0.ffn_l.1.ffn_1.bias', 'decoder.cross_blocks.0.0.ffn_l.1.ffn_2.weight', 'decoder.cross_blocks.0.0.ffn_l.1.ffn_2.bias', 'decoder.cross_blocks.0.0.ffn_l.1.layer_norm.weight', 'decoder.cross_blocks.0.0.ffn_l.1.layer_norm.bias', 'decoder.cross_blocks.0.0.attn_l.0.relative_position_bias_table', 'decoder.cross_blocks.0.0.attn_l.0.relative_position_index', 'decoder.cross_blocks.0.0.attn_l.0.q_proj.weight', 'decoder.cross_blocks.0.0.attn_l.0.kv_proj.weight', 'decoder.cross_blocks.0.0.attn_l.0.proj.weight', 'decoder.cross_blocks.0.0.attn_l.0.proj.bias', 'decoder.cross_blocks.0.0.attn_l.0.norm.weight', 'decoder.cross_blocks.0.0.attn_l.0.norm.bias', 'decoder.cross_blocks.0.0.attn_l.1.relative_position_bias_table', 'decoder.cross_blocks.0.0.attn_l.1.relative_position_index', 'decoder.cross_blocks.0.0.attn_l.1.q_proj.weight', 'decoder.cross_blocks.0.0.attn_l.1.kv_proj.weight', 'decoder.cross_blocks.0.0.attn_l.1.proj.weight', 'decoder.cross_blocks.0.0.attn_l.1.proj.bias', 'decoder.cross_blocks.0.0.attn_l.1.norm.weight', 'decoder.cross_blocks.0.0.attn_l.1.norm.bias', 'decoder.cross_blocks.1.0.ffn_l.0.ffn_1.weight', 'decoder.cross_blocks.1.0.ffn_l.0.ffn_1.bias', 'decoder.cross_blocks.1.0.ffn_l.0.ffn_2.weight', 'decoder.cross_blocks.1.0.ffn_l.0.ffn_2.bias', 'decoder.cross_blocks.1.0.ffn_l.0.layer_norm.weight', 'decoder.cross_blocks.1.0.ffn_l.0.layer_norm.bias', 'decoder.cross_blocks.1.0.ffn_l.1.ffn_1.weight', 'decoder.cross_blocks.1.0.ffn_l.1.ffn_1.bias', 'decoder.cross_blocks.1.0.ffn_l.1.ffn_2.weight', 'decoder.cross_blocks.1.0.ffn_l.1.ffn_2.bias', 'decoder.cross_blocks.1.0.ffn_l.1.layer_norm.weight', 'decoder.cross_blocks.1.0.ffn_l.1.layer_norm.bias', 'decoder.cross_blocks.1.0.attn_l.0.relative_position_bias_table', 'decoder.cross_blocks.1.0.attn_l.0.relative_position_index', 'decoder.cross_blocks.1.0.attn_l.0.q_proj.weight', 'decoder.cross_blocks.1.0.attn_l.0.kv_proj.weight', 'decoder.cross_blocks.1.0.attn_l.0.proj.weight', 'decoder.cross_blocks.1.0.attn_l.0.proj.bias', 'decoder.cross_blocks.1.0.attn_l.0.norm.weight', 'decoder.cross_blocks.1.0.attn_l.0.norm.bias', 'decoder.cross_blocks.1.0.attn_l.1.relative_position_bias_table', 'decoder.cross_blocks.1.0.attn_l.1.relative_position_index', 'decoder.cross_blocks.1.0.attn_l.1.q_proj.weight', 'decoder.cross_blocks.1.0.attn_l.1.kv_proj.weight', 'decoder.cross_blocks.1.0.attn_l.1.proj.weight', 'decoder.cross_blocks.1.0.attn_l.1.proj.bias', 'decoder.cross_blocks.1.0.attn_l.1.norm.weight', 'decoder.cross_blocks.1.0.attn_l.1.norm.bias']\n",
      "Unexpected keys:\n",
      "[]\n",
      "36\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "load_result = EFmodel.load_state_dict(compatible_state_dict, strict=False)\n",
    "print(\"Missing keys:\")\n",
    "print(load_result.missing_keys)\n",
    "print(\"Unexpected keys:\")\n",
    "print(load_result.unexpected_keys)\n",
    "print(len(compatible_state_dict)-len(load_result.missing_keys))\n",
    "print(len(compatible_state_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47455bbb",
   "metadata": {},
   "source": [
    "# 5. Classifier Head Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27259f1d",
   "metadata": {},
   "source": [
    "## 5.1 Wrap Earthformer into classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74a311",
   "metadata": {},
   "source": [
    "Define classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a595a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthformerClassifier(nn.Module):\n",
    "    def __init__(self, earthformer_model, num_classes=8):\n",
    "        super().__init__()\n",
    "        self.model = earthformer_model\n",
    "        # upscale C to a number of latent dims\n",
    "        self.pool = nn.AdaptiveAvgPool3d((1, 1, 1))  # Pool over T, H, W\n",
    "        # TODO ablation study with max pooling?\n",
    "        self.classifier = nn.Linear(self.model.target_shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)  # (B, T_out, H, W, C_out)\n",
    "        x = x.permute(0, 4, 1, 2, 3)  # â†’ [B, C_out, T_out, H, W]\n",
    "        x = self.pool(x).squeeze()    # â†’ [B, C_out]\n",
    "        logits = self.classifier(x)   # â†’ [B, num_classes]\n",
    "        probs = torch.sigmoid(logits) if logits.shape[1] == 1 else torch.softmax(logits, dim=1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c26511",
   "metadata": {},
   "source": [
    "Instantiate classifier with EF model from previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "20e700f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 8\n",
    "EFClassifier = EarthformerClassifier(EFmodel, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c732d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "EFClassifier.eval()\n",
    "x = EFClassifier(x_final[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b2a3df52",
   "metadata": {},
   "outputs": [],
   "source": [
    "EFmodel.eval()\n",
    "x2 = EFmodel(x_final[0:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845c174b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 128, 128, 64])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(x2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f6ff1219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81c8cb",
   "metadata": {},
   "source": [
    "## 5.2 Freeze pretrained layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67262e6",
   "metadata": {},
   "source": [
    "Freeze everything except classifier head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881014f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in EFmodel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in EFClassifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1475b",
   "metadata": {},
   "source": [
    "Freeze encoder/decoder blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24942c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in EFModel.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in EFModel.decoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345874e9",
   "metadata": {},
   "source": [
    "# 6. Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71df4f6",
   "metadata": {},
   "source": [
    "## 6.1 Define loss & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ebbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss for classification\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "lr = 5e-4  # needs to be adjusted if finetuning\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5166af8",
   "metadata": {},
   "source": [
    "## 6.2 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aecb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, batch_x, batch_y):\n",
    "    model.train()\n",
    "    logits = model(batch_x)\n",
    "    loss = loss_func(logits, batch_y)\n",
    "    return loss\n",
    "\n",
    "def validation_step(model, batch_x, batch_y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_x)\n",
    "        loss = loss_func(logits, batch_y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == batch_y).float().mean()\n",
    "    return loss.item(), acc.item()\n",
    "\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            logits = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f3395c",
   "metadata": {},
   "source": [
    "# 7. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8729049",
   "metadata": {},
   "source": [
    "Train the classifier over an appropriate number of epochs, log training and validation loss (and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        loss = training_step(EFClassifier, x_batch, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    for x_batch, y_batch in val_loader:\n",
    "        batch_loss, batch_acc = validation_step(EFClassifier, x_batch, y_batch)\n",
    "        val_loss += batch_loss\n",
    "        val_acc += batch_acc\n",
    "    \n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accuracies.append(val_acc / len(val_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f} | Val Acc: {val_accuracies[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a18189",
   "metadata": {},
   "source": [
    "# 8. Results & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "697e2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot loss/accuracy vs. epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e70ac",
   "metadata": {},
   "source": [
    "# 9. Save Model & Export Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"outputs/{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model and optimizer states\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}, os.path.join(output_dir, \"checkpoint.pt\"))\n",
    "\n",
    "# Save loss history\n",
    "with open(os.path.join(output_dir, \"train_loss.json\"), \"w\") as f:\n",
    "    json.dump(train_loss_history, f)\n",
    "with open(os.path.join(output_dir, \"val_loss.json\"), \"w\") as f:\n",
    "    json.dump(val_loss_history, f)\n",
    "\n",
    "# Plot and save loss curves\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, \"loss_curve.png\"))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

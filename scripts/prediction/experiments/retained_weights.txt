===================================================
In shape: [10, 128, 128, 4]
Out shape: [20, 128, 128, 4]
===================================================

When loading from:
model = CuboidTransformerModel(input_shape=[10, 128, 128, 4],
                               target_shape=[20, 128, 128, 4],
                               **earthformer_config)

Loading: initial_encoder.conv_block_list.0.0.weight | with shape: torch.Size([64, 4, 3, 3])
Loading: initial_encoder.conv_block_list.0.0.bias | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.0.1.weight | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.0.1.bias | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.0.3.weight | with shape: torch.Size([64, 64, 3, 3])
Loading: initial_encoder.conv_block_list.0.3.bias | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.0.4.weight | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.0.4.bias | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.1.0.weight | with shape: torch.Size([256, 64, 3, 3])
Loading: initial_encoder.conv_block_list.1.0.bias | with shape: torch.Size([256])
Loading: initial_encoder.conv_block_list.1.1.weight | with shape: torch.Size([256])
Loading: initial_encoder.conv_block_list.1.1.bias | with shape: torch.Size([256])
Loading: initial_encoder.conv_block_list.1.3.weight | with shape: torch.Size([256, 256, 3, 3])
Loading: initial_encoder.conv_block_list.1.3.bias | with shape: torch.Size([256])
Loading: initial_encoder.conv_block_list.1.4.weight | with shape: torch.Size([256])
Loading: initial_encoder.conv_block_list.1.4.bias | with shape: torch.Size([256])
Loading: initial_encoder.patch_merge_list.0.reduction.weight | with shape: torch.Size([64, 256])
Loading: initial_encoder.patch_merge_list.0.norm.weight | with shape: torch.Size([256])
Loading: initial_encoder.patch_merge_list.0.norm.bias | with shape: torch.Size([256])
Loading: initial_encoder.patch_merge_list.1.reduction.weight | with shape: torch.Size([256, 1024])
Loading: initial_encoder.patch_merge_list.1.norm.weight | with shape: torch.Size([1024])
Loading: initial_encoder.patch_merge_list.1.norm.bias | with shape: torch.Size([1024])
Loading: final_decoder.conv_block_list.0.0.weight | with shape: torch.Size([256, 256, 3, 3])
Loading: final_decoder.conv_block_list.0.0.bias | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.0.1.weight | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.0.1.bias | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.0.3.weight | with shape: torch.Size([256, 256, 3, 3])
Loading: final_decoder.conv_block_list.0.3.bias | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.0.4.weight | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.0.4.bias | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.1.0.weight | with shape: torch.Size([64, 256, 3, 3])
Loading: final_decoder.conv_block_list.1.0.bias | with shape: torch.Size([64])
Loading: final_decoder.conv_block_list.1.1.weight | with shape: torch.Size([64])
Loading: final_decoder.conv_block_list.1.1.bias | with shape: torch.Size([64])
Loading: final_decoder.conv_block_list.1.3.weight | with shape: torch.Size([64, 64, 3, 3])
Loading: final_decoder.conv_block_list.1.3.bias | with shape: torch.Size([64])
Loading: final_decoder.conv_block_list.1.4.weight | with shape: torch.Size([64])
Loading: final_decoder.conv_block_list.1.4.bias | with shape: torch.Size([64])
Loading: final_decoder.upsample_list.0.conv.weight | with shape: torch.Size([256, 256, 3, 3])
Loading: final_decoder.upsample_list.0.conv.bias | with shape: torch.Size([256])
Loading: final_decoder.upsample_list.1.conv.weight | with shape: torch.Size([256, 256, 3, 3])
Loading: final_decoder.upsample_list.1.conv.bias | with shape: torch.Size([256])
Loading: dec_final_proj.weight | with shape: torch.Size([4, 64])
Loading: dec_final_proj.bias | with shape: torch.Size([4])
Loading: encoder.down_layers.0.reduction.weight | with shape: torch.Size([512, 1024])
Loading: encoder.down_layers.0.norm.weight | with shape: torch.Size([1024])
Loading: encoder.down_layers.0.norm.bias | with shape: torch.Size([1024])
Loading: encoder.down_layer_global_proj.0.weight | with shape: torch.Size([512, 256])
Loading: encoder.down_layer_global_proj.0.bias | with shape: torch.Size([512])
Loading: encoder.blocks.0.0.ffn_l.0.ffn_1.weight | with shape: torch.Size([1024, 256])
Loading: encoder.blocks.0.0.ffn_l.0.ffn_1.bias | with shape: torch.Size([1024])
Loading: encoder.blocks.0.0.ffn_l.0.ffn_2.weight | with shape: torch.Size([256, 1024])
Loading: encoder.blocks.0.0.ffn_l.0.ffn_2.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.ffn_l.0.layer_norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.ffn_l.0.layer_norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.ffn_l.1.ffn_1.weight | with shape: torch.Size([1024, 256])
Loading: encoder.blocks.0.0.ffn_l.1.ffn_1.bias | with shape: torch.Size([1024])
Loading: encoder.blocks.0.0.ffn_l.1.ffn_2.weight | with shape: torch.Size([256, 1024])
Loading: encoder.blocks.0.0.ffn_l.1.ffn_2.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.ffn_l.1.layer_norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.ffn_l.1.layer_norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.qkv.weight | with shape: torch.Size([768, 256])
Loading: encoder.blocks.0.0.attn_l.0.l2g_q_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.l2g_global_kv_net.weight | with shape: torch.Size([512, 256])
Loading: encoder.blocks.0.0.attn_l.0.g2l_global_q_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.g2l_k_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.g2l_v_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.g2g_global_qkv_net.weight | with shape: torch.Size([768, 256])
Loading: encoder.blocks.0.0.attn_l.0.proj.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.proj.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.global_proj.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.global_proj.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.global_vec_norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.global_vec_norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.qkv.weight | with shape: torch.Size([768, 256])
Loading: encoder.blocks.0.0.attn_l.1.l2g_q_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.l2g_global_kv_net.weight | with shape: torch.Size([512, 256])
Loading: encoder.blocks.0.0.attn_l.1.g2l_global_q_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.g2l_k_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.g2l_v_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.g2g_global_qkv_net.weight | with shape: torch.Size([768, 256])
Loading: encoder.blocks.0.0.attn_l.1.proj.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.proj.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.global_proj.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.global_proj.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.global_vec_norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.global_vec_norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.1.0.ffn_l.0.ffn_1.weight | with shape: torch.Size([2048, 512])
Loading: encoder.blocks.1.0.ffn_l.0.ffn_1.bias | with shape: torch.Size([2048])
Loading: encoder.blocks.1.0.ffn_l.0.ffn_2.weight | with shape: torch.Size([512, 2048])
Loading: encoder.blocks.1.0.ffn_l.0.ffn_2.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.ffn_l.0.layer_norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.ffn_l.0.layer_norm.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.ffn_l.1.ffn_1.weight | with shape: torch.Size([2048, 512])
Loading: encoder.blocks.1.0.ffn_l.1.ffn_1.bias | with shape: torch.Size([2048])
Loading: encoder.blocks.1.0.ffn_l.1.ffn_2.weight | with shape: torch.Size([512, 2048])
Loading: encoder.blocks.1.0.ffn_l.1.ffn_2.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.ffn_l.1.layer_norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.ffn_l.1.layer_norm.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.qkv.weight | with shape: torch.Size([1536, 512])
Loading: encoder.blocks.1.0.attn_l.0.l2g_q_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.l2g_global_kv_net.weight | with shape: torch.Size([1024, 512])
Loading: encoder.blocks.1.0.attn_l.0.g2l_global_q_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.g2l_k_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.g2l_v_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.g2g_global_qkv_net.weight | with shape: torch.Size([1536, 512])
Loading: encoder.blocks.1.0.attn_l.0.proj.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.proj.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.global_proj.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.global_proj.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.norm.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.global_vec_norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.global_vec_norm.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.qkv.weight | with shape: torch.Size([1536, 512])
Loading: encoder.blocks.1.0.attn_l.1.l2g_q_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.l2g_global_kv_net.weight | with shape: torch.Size([1024, 512])
Loading: encoder.blocks.1.0.attn_l.1.g2l_global_q_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.g2l_k_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.g2l_v_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.g2g_global_qkv_net.weight | with shape: torch.Size([1536, 512])
Loading: encoder.blocks.1.0.attn_l.1.proj.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.proj.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.global_proj.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.global_proj.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.norm.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.global_vec_norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.global_vec_norm.bias | with shape: torch.Size([512])
Loading: enc_pos_embed.T_embed.weight | with shape: torch.Size([10, 256])
Loading: enc_pos_embed.HW_embed.weight | with shape: torch.Size([1024, 256])
Loading: dec_pos_embed.T_embed.weight | with shape: torch.Size([20, 512])
Loading: dec_pos_embed.HW_embed.weight | with shape: torch.Size([256, 512])
Loading: decoder.upsample_layers.0.conv.weight | with shape: torch.Size([256, 512, 3, 3])
Loading: decoder.upsample_layers.0.conv.bias | with shape: torch.Size([256])
===================================================
Skipping: init_global_vectors | pretrained shape: torch.Size([8, 256]) vs model shape: torch.Size([2, 256])
Skipping: initial_aux_encoder.conv_block_list.0.0.weight | pretrained shape: torch.Size([64, 7, 3, 3]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.0.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.1.weight | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.1.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.3.weight | pretrained shape: torch.Size([64, 64, 3, 3]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.3.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.4.weight | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.4.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.0.weight | pretrained shape: torch.Size([256, 64, 3, 3]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.0.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.1.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.1.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.3.weight | pretrained shape: torch.Size([256, 256, 3, 3]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.3.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.4.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.4.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.0.reduction.weight | pretrained shape: torch.Size([64, 256]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.1.reduction.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.1.norm.weight | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.1.norm.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: torch.Size([343, 4])
Skipping: encoder.blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: torch.Size([64, 64])
Skipping: encoder.blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: torch.Size([343, 4])
Skipping: encoder.blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: torch.Size([64, 64])
Skipping: encoder.blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.l2g_q_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.l2g_global_kv_net.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.g2l_global_q_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.g2l_k_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.g2l_v_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.g2g_global_qkv_net.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.global_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.global_proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.global_vec_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.global_vec_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: torch.Size([343, 4])
Skipping: encoder.blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: torch.Size([64, 64])
Skipping: encoder.blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: torch.Size([343, 4])
Skipping: encoder.blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: torch.Size([64, 64])
Skipping: encoder.blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.l2g_q_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.l2g_global_kv_net.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.g2l_global_q_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.g2l_k_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.g2l_v_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.g2g_global_qkv_net.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.global_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.global_proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.global_vec_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.global_vec_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.downsample_layers.0.reduction.weight | pretrained shape: torch.Size([512, 1024]) vs model shape: MISSING
Skipping: decoder.downsample_layers.0.norm.weight | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.downsample_layers.0.norm.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING

===================================================
In shape: [1, 128, 128, 2]
Out shape: [1, 128, 128, 2]
===================================================

When loading from:
model = CuboidTransformerModel(input_shape=[1, 128, 128, 2],
                               target_shape=[1, 128, 128, 2],
                               **earthformer_config)

Loading: initial_encoder.conv_block_list.0.0.bias | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.0.1.weight | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.0.1.bias | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.0.3.weight | with shape: torch.Size([64, 64, 3, 3])
Loading: initial_encoder.conv_block_list.0.3.bias | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.0.4.weight | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.0.4.bias | with shape: torch.Size([64])
Loading: initial_encoder.conv_block_list.1.0.weight | with shape: torch.Size([256, 64, 3, 3])
Loading: initial_encoder.conv_block_list.1.0.bias | with shape: torch.Size([256])
Loading: initial_encoder.conv_block_list.1.1.weight | with shape: torch.Size([256])
Loading: initial_encoder.conv_block_list.1.1.bias | with shape: torch.Size([256])
Loading: initial_encoder.conv_block_list.1.3.weight | with shape: torch.Size([256, 256, 3, 3])
Loading: initial_encoder.conv_block_list.1.3.bias | with shape: torch.Size([256])
Loading: initial_encoder.conv_block_list.1.4.weight | with shape: torch.Size([256])
Loading: initial_encoder.conv_block_list.1.4.bias | with shape: torch.Size([256])
Loading: initial_encoder.patch_merge_list.0.reduction.weight | with shape: torch.Size([64, 256])
Loading: initial_encoder.patch_merge_list.0.norm.weight | with shape: torch.Size([256])
Loading: initial_encoder.patch_merge_list.0.norm.bias | with shape: torch.Size([256])
Loading: initial_encoder.patch_merge_list.1.reduction.weight | with shape: torch.Size([256, 1024])
Loading: initial_encoder.patch_merge_list.1.norm.weight | with shape: torch.Size([1024])
Loading: initial_encoder.patch_merge_list.1.norm.bias | with shape: torch.Size([1024])
Loading: final_decoder.conv_block_list.0.0.weight | with shape: torch.Size([256, 256, 3, 3])
Loading: final_decoder.conv_block_list.0.0.bias | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.0.1.weight | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.0.1.bias | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.0.3.weight | with shape: torch.Size([256, 256, 3, 3])
Loading: final_decoder.conv_block_list.0.3.bias | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.0.4.weight | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.0.4.bias | with shape: torch.Size([256])
Loading: final_decoder.conv_block_list.1.0.weight | with shape: torch.Size([64, 256, 3, 3])
Loading: final_decoder.conv_block_list.1.0.bias | with shape: torch.Size([64])
Loading: final_decoder.conv_block_list.1.1.weight | with shape: torch.Size([64])
Loading: final_decoder.conv_block_list.1.1.bias | with shape: torch.Size([64])
Loading: final_decoder.conv_block_list.1.3.weight | with shape: torch.Size([64, 64, 3, 3])
Loading: final_decoder.conv_block_list.1.3.bias | with shape: torch.Size([64])
Loading: final_decoder.conv_block_list.1.4.weight | with shape: torch.Size([64])
Loading: final_decoder.conv_block_list.1.4.bias | with shape: torch.Size([64])
Loading: final_decoder.upsample_list.0.conv.weight | with shape: torch.Size([256, 256, 3, 3])
Loading: final_decoder.upsample_list.0.conv.bias | with shape: torch.Size([256])
Loading: final_decoder.upsample_list.1.conv.weight | with shape: torch.Size([256, 256, 3, 3])
Loading: final_decoder.upsample_list.1.conv.bias | with shape: torch.Size([256])
Loading: encoder.down_layers.0.reduction.weight | with shape: torch.Size([512, 1024])
Loading: encoder.down_layers.0.norm.weight | with shape: torch.Size([1024])
Loading: encoder.down_layers.0.norm.bias | with shape: torch.Size([1024])
Loading: encoder.down_layer_global_proj.0.weight | with shape: torch.Size([512, 256])
Loading: encoder.down_layer_global_proj.0.bias | with shape: torch.Size([512])
Loading: encoder.blocks.0.0.ffn_l.0.ffn_1.weight | with shape: torch.Size([1024, 256])
Loading: encoder.blocks.0.0.ffn_l.0.ffn_1.bias | with shape: torch.Size([1024])
Loading: encoder.blocks.0.0.ffn_l.0.ffn_2.weight | with shape: torch.Size([256, 1024])
Loading: encoder.blocks.0.0.ffn_l.0.ffn_2.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.ffn_l.0.layer_norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.ffn_l.0.layer_norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.ffn_l.1.ffn_1.weight | with shape: torch.Size([1024, 256])
Loading: encoder.blocks.0.0.ffn_l.1.ffn_1.bias | with shape: torch.Size([1024])
Loading: encoder.blocks.0.0.ffn_l.1.ffn_2.weight | with shape: torch.Size([256, 1024])
Loading: encoder.blocks.0.0.ffn_l.1.ffn_2.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.ffn_l.1.layer_norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.ffn_l.1.layer_norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.qkv.weight | with shape: torch.Size([768, 256])
Loading: encoder.blocks.0.0.attn_l.0.l2g_q_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.l2g_global_kv_net.weight | with shape: torch.Size([512, 256])
Loading: encoder.blocks.0.0.attn_l.0.g2l_global_q_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.g2l_k_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.g2l_v_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.g2g_global_qkv_net.weight | with shape: torch.Size([768, 256])
Loading: encoder.blocks.0.0.attn_l.0.proj.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.proj.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.global_proj.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.0.global_proj.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.global_vec_norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.0.global_vec_norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.qkv.weight | with shape: torch.Size([768, 256])
Loading: encoder.blocks.0.0.attn_l.1.l2g_q_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.l2g_global_kv_net.weight | with shape: torch.Size([512, 256])
Loading: encoder.blocks.0.0.attn_l.1.g2l_global_q_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.g2l_k_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.g2l_v_net.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.g2g_global_qkv_net.weight | with shape: torch.Size([768, 256])
Loading: encoder.blocks.0.0.attn_l.1.proj.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.proj.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.global_proj.weight | with shape: torch.Size([256, 256])
Loading: encoder.blocks.0.0.attn_l.1.global_proj.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.global_vec_norm.weight | with shape: torch.Size([256])
Loading: encoder.blocks.0.0.attn_l.1.global_vec_norm.bias | with shape: torch.Size([256])
Loading: encoder.blocks.1.0.ffn_l.0.ffn_1.weight | with shape: torch.Size([2048, 512])
Loading: encoder.blocks.1.0.ffn_l.0.ffn_1.bias | with shape: torch.Size([2048])
Loading: encoder.blocks.1.0.ffn_l.0.ffn_2.weight | with shape: torch.Size([512, 2048])
Loading: encoder.blocks.1.0.ffn_l.0.ffn_2.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.ffn_l.0.layer_norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.ffn_l.0.layer_norm.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.ffn_l.1.ffn_1.weight | with shape: torch.Size([2048, 512])
Loading: encoder.blocks.1.0.ffn_l.1.ffn_1.bias | with shape: torch.Size([2048])
Loading: encoder.blocks.1.0.ffn_l.1.ffn_2.weight | with shape: torch.Size([512, 2048])
Loading: encoder.blocks.1.0.ffn_l.1.ffn_2.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.ffn_l.1.layer_norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.ffn_l.1.layer_norm.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.qkv.weight | with shape: torch.Size([1536, 512])
Loading: encoder.blocks.1.0.attn_l.0.l2g_q_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.l2g_global_kv_net.weight | with shape: torch.Size([1024, 512])
Loading: encoder.blocks.1.0.attn_l.0.g2l_global_q_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.g2l_k_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.g2l_v_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.g2g_global_qkv_net.weight | with shape: torch.Size([1536, 512])
Loading: encoder.blocks.1.0.attn_l.0.proj.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.proj.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.global_proj.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.0.global_proj.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.norm.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.global_vec_norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.0.global_vec_norm.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.qkv.weight | with shape: torch.Size([1536, 512])
Loading: encoder.blocks.1.0.attn_l.1.l2g_q_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.l2g_global_kv_net.weight | with shape: torch.Size([1024, 512])
Loading: encoder.blocks.1.0.attn_l.1.g2l_global_q_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.g2l_k_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.g2l_v_net.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.g2g_global_qkv_net.weight | with shape: torch.Size([1536, 512])
Loading: encoder.blocks.1.0.attn_l.1.proj.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.proj.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.global_proj.weight | with shape: torch.Size([512, 512])
Loading: encoder.blocks.1.0.attn_l.1.global_proj.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.norm.bias | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.global_vec_norm.weight | with shape: torch.Size([512])
Loading: encoder.blocks.1.0.attn_l.1.global_vec_norm.bias | with shape: torch.Size([512])
Loading: enc_pos_embed.HW_embed.weight | with shape: torch.Size([1024, 256])
Loading: dec_pos_embed.HW_embed.weight | with shape: torch.Size([256, 512])
Loading: decoder.upsample_layers.0.conv.weight | with shape: torch.Size([256, 512, 3, 3])
Loading: decoder.upsample_layers.0.conv.bias | with shape: torch.Size([256])
===================================================
Skipping: init_global_vectors | pretrained shape: torch.Size([8, 256]) vs model shape: torch.Size([2, 256])
Skipping: initial_encoder.conv_block_list.0.0.weight | pretrained shape: torch.Size([64, 4, 3, 3]) vs model shape: torch.Size([64, 2, 3, 3])
Skipping: initial_aux_encoder.conv_block_list.0.0.weight | pretrained shape: torch.Size([64, 7, 3, 3]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.0.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.1.weight | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.1.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.3.weight | pretrained shape: torch.Size([64, 64, 3, 3]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.3.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.4.weight | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.0.4.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.0.weight | pretrained shape: torch.Size([256, 64, 3, 3]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.0.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.1.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.1.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.3.weight | pretrained shape: torch.Size([256, 256, 3, 3]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.3.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.4.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.conv_block_list.1.4.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.0.reduction.weight | pretrained shape: torch.Size([64, 256]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.1.reduction.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.1.norm.weight | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: initial_aux_encoder.patch_merge_list.1.norm.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: dec_final_proj.weight | pretrained shape: torch.Size([4, 64]) vs model shape: torch.Size([2, 64])
Skipping: dec_final_proj.bias | pretrained shape: torch.Size([4]) vs model shape: torch.Size([2])
Skipping: encoder.blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: torch.Size([343, 4])
Skipping: encoder.blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: torch.Size([64, 64])
Skipping: encoder.blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: torch.Size([343, 4])
Skipping: encoder.blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: torch.Size([64, 64])
Skipping: encoder.blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.l2g_q_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.l2g_global_kv_net.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.g2l_global_q_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.g2l_k_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.g2l_v_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.g2g_global_qkv_net.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.global_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.global_proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.global_vec_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.0.0.attn_l.2.global_vec_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: torch.Size([343, 4])
Skipping: encoder.blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: torch.Size([64, 64])
Skipping: encoder.blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: torch.Size([343, 4])
Skipping: encoder.blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: torch.Size([64, 64])
Skipping: encoder.blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.l2g_q_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.l2g_global_kv_net.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.g2l_global_q_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.g2l_k_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.g2l_v_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.g2g_global_qkv_net.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.global_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.global_proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.global_vec_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: encoder.blocks.1.0.attn_l.2.global_vec_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: enc_pos_embed.T_embed.weight | pretrained shape: torch.Size([10, 256]) vs model shape: torch.Size([1, 256])
Skipping: dec_pos_embed.T_embed.weight | pretrained shape: torch.Size([20, 512]) vs model shape: torch.Size([1, 512])
Skipping: decoder.downsample_layers.0.reduction.weight | pretrained shape: torch.Size([512, 1024]) vs model shape: MISSING
Skipping: decoder.downsample_layers.0.norm.weight | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.downsample_layers.0.norm.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.1.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.1.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_self_blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.1.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.1.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_self_blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.up_cross_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING
Skipping: decoder.down_cross_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ede748",
   "metadata": {},
   "source": [
    "# Predicting Archetypes with Earthformer\n",
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37b4b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import xarray as xr\n",
    "\n",
    "# import both nc's\n",
    "stream_path = \"../../data/deseason_smsub/lentis_stream250_JJA_2deg_101_deseason_spatialsub.nc\"\n",
    "dataset_stream = xr.open_dataset(stream_path)\n",
    "\n",
    "tas_path = \"../../data/deseason_smsub/lentis_tas_JJA_2deg_101_deseason.nc\"\n",
    "dataset_tas = xr.open_dataset(tas_path)\n",
    "\n",
    "# get S_PCHA from archetypes file\n",
    "with h5py.File('../../data/deseason_smsub/pcha_results_8a.hdf5', 'r') as f: # run from mmi393 directory or gives error\n",
    "        S_PCHA = f['/S_PCHA'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c76dc13",
   "metadata": {},
   "source": [
    "Join TAS and stream function data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2991d908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3480835\n",
      "0.13922119\n",
      "-0.3480835\n",
      "0.13922119\n"
     ]
    }
   ],
   "source": [
    "# group indices based on whichever archetype is maximum there\n",
    "arch_indices = np.argmax(S_PCHA, axis=0)\n",
    "\n",
    "# sanity check part 1: these results should be the same in part 2\n",
    "print(dataset_tas.isel(time=123)['tas'].isel(lon=0, lat=0).values)\n",
    "print(dataset_tas.isel(time=74)['tas'].isel(lon=4, lat=8).values)\n",
    "\n",
    "# join the nc's together\n",
    "dataset_comb = dataset_stream.assign(tas=dataset_tas['tas'])\n",
    "\n",
    "# sanity check part 2\n",
    "print(dataset_comb.isel(time=123)['tas'].isel(lon=0, lat=0).values)\n",
    "print(dataset_comb.isel(time=74)['tas'].isel(lon=4, lat=8).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45647cd",
   "metadata": {},
   "source": [
    "Add labels from AA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "addde090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 2 3\n",
      "4 2 2 3\n",
      "4 2 2 3\n"
     ]
    }
   ],
   "source": [
    "# sanity check 1\n",
    "print(arch_indices[0], arch_indices[5], arch_indices[6], arch_indices[9119])\n",
    "\n",
    "arch_da = xr.DataArray(arch_indices, dims=\"time\", coords={\"time\": dataset_comb.time})\n",
    "# sanity check 2 \n",
    "print(arch_da.isel(time=0).values, arch_da.isel(time=5).values, arch_da.isel(time=6).values, arch_da.isel(time=9119).values)\n",
    "\n",
    "# calculate the mean for each archetype's group\n",
    "dataset_comb_labeled = dataset_comb.assign(archetype=arch_da)\n",
    "\n",
    "# sanity check 3\n",
    "print(dataset_comb_labeled.isel(time=0)['archetype'].values,\n",
    "      dataset_comb_labeled.isel(time=5)['archetype'].values,\n",
    "      dataset_comb_labeled.isel(time=6)['archetype'].values,\n",
    "      dataset_comb_labeled.isel(time=9119)['archetype'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593acae1",
   "metadata": {},
   "source": [
    "## Dataset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4bd6bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scistor/ivm/mmi393/.conda/envs/netcdf_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04c65e",
   "metadata": {},
   "source": [
    "From xarray dataset to pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc6866d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9200, 29, 170, 2])\n"
     ]
    }
   ],
   "source": [
    "stream = dataset_comb['stream'].squeeze('plev').values  # (T, lat, lon)\n",
    "tas = dataset_comb['tas'].values                        # (T, lat, lon)\n",
    "\n",
    "# Extract and squeeze stream function\n",
    "stream = dataset_comb['stream'].squeeze('plev').values  # (T, H, W)\n",
    "tas = dataset_comb['tas'].values                        # (T, H, W)\n",
    "\n",
    "# Stack the variables along the channel axis\n",
    "x_np = np.stack([stream, tas], axis=-1)  # shape: (T, H, W, C) where C = 2\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "x_tensor = torch.from_numpy(x_np).float()\n",
    "\n",
    "print(x_tensor.shape)  # (T=9200, H=29, W=170, C=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d8b77",
   "metadata": {},
   "source": [
    "Target construction\n",
    "\n",
    "If t+7 is belongs to another year, exclude example from labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58ececb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_final shape: torch.Size([8500, 29, 170, 2])\n",
      "y_final shape: torch.Size([8500])\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "l = 7  # lead time\n",
    "time = dataset_comb['time'].values  # format: datetime64\n",
    "arch_labels = arch_da.values        # (9200,)\n",
    "\n",
    "x_all = x_tensor  # shape: (T, H, W, C)\n",
    "x_list = []\n",
    "y_list = []\n",
    "kept_time_indices = []\n",
    "\n",
    "# Makes it so that examples from different years do not get combined\n",
    "# TODO Add data from September to include last week of August?\n",
    "for t in range(len(time) - l):\n",
    "    target_time = time[t] + np.timedelta64(l, 'D')\n",
    "    if time[t + l] == target_time:\n",
    "        x_list.append(x_all[t])\n",
    "        y_list.append(arch_labels[t + l])\n",
    "        kept_time_indices.append(t)\n",
    "\n",
    "# Stack into tensors\n",
    "x_final = torch.stack(x_list)              # shape: (N, H, W, C)\n",
    "y_final = torch.tensor(y_list, dtype=torch.long)  # shape: (N,)\n",
    "\n",
    "print(f\"x_final shape: {x_final.shape}\") # approx. 8% of the dataset is cut\n",
    "print(f\"y_final shape: {y_final.shape}\")\n",
    "print(kept_time_indices[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedebd74",
   "metadata": {},
   "source": [
    "Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb05717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO split x&y into train/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17678ec",
   "metadata": {},
   "source": [
    "## Using Earthformer\n",
    "Import pretrained Earthformer checkpoint for EarthNet2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42d45dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from earthformer.cuboid_transformer.cuboid_transformer import CuboidTransformerModel\n",
    "#from earthformer.train_cuboid_earthnet import CuboidEarthNet2021PLModule\n",
    "from earthformer.utils.utils import download\n",
    "\n",
    "save_dir = \"./experiments\"\n",
    "\n",
    "pretrained_checkpoint_url = \"https://earthformer.s3.amazonaws.com/pretrained_checkpoints/earthformer_earthnet2021.pt\"\n",
    "local_checkpoint_path = os.path.join(save_dir, \"earthformer_earthnet2021.pt\")\n",
    "download(url=pretrained_checkpoint_url, path=local_checkpoint_path)\n",
    "\n",
    "state_dict = torch.load(local_checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "#pl_module.torch_nn_module.load_state_dict(state_dict=state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f1e064",
   "metadata": {},
   "source": [
    "Initialize Earthformer model with the correct config, based on Earthnet2021 setup in Earthformer github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cfa4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthformer_config = {\n",
    "    \"base_units\": 256,\n",
    "    \"block_units\": None,\n",
    "    \"scale_alpha\": 1.0,\n",
    "\n",
    "    \"enc_depth\": [1, 1],\n",
    "    \"dec_depth\": [1, 1],\n",
    "    \"enc_use_inter_ffn\": True,\n",
    "    \"dec_use_inter_ffn\": True,\n",
    "    \"dec_hierarchical_pos_embed\": False,\n",
    "\n",
    "    \"downsample\": 2,\n",
    "    \"downsample_type\": \"patch_merge\",\n",
    "    \"upsample_type\": \"upsample\",\n",
    "\n",
    "    \"num_global_vectors\": 2,\n",
    "    \"use_dec_self_global\": False,\n",
    "    \"dec_self_update_global\": True,\n",
    "    \"use_dec_cross_global\": False,\n",
    "    \"use_global_vector_ffn\": False,\n",
    "    \"use_global_self_attn\": True,\n",
    "    \"separate_global_qkv\": True,\n",
    "    \"global_dim_ratio\": 1,\n",
    "\n",
    "    \"attn_drop\": 0.1,\n",
    "    \"proj_drop\": 0.1,\n",
    "    \"ffn_drop\": 0.1,\n",
    "    \"num_heads\": 4,\n",
    "\n",
    "    \"ffn_activation\": \"gelu\",\n",
    "    \"gated_ffn\": False,\n",
    "    \"norm_layer\": \"layer_norm\",\n",
    "    \"padding_type\": \"zeros\",\n",
    "    \"pos_embed_type\": \"t+hw\",\n",
    "    \"use_relative_pos\": True,\n",
    "    \"self_attn_use_final_proj\": True,\n",
    "\n",
    "    \"checkpoint_level\": 0,\n",
    "\n",
    "    \"initial_downsample_type\": \"stack_conv\",\n",
    "    \"initial_downsample_activation\": \"leaky\",\n",
    "    \"initial_downsample_stack_conv_num_layers\": 2,\n",
    "    \"initial_downsample_stack_conv_dim_list\": [64, 256],\n",
    "    \"initial_downsample_stack_conv_downscale_list\": [2, 2],\n",
    "    \"initial_downsample_stack_conv_num_conv_list\": [2, 2],\n",
    "\n",
    "    \"attn_linear_init_mode\": \"0\",\n",
    "    \"ffn_linear_init_mode\": \"0\",\n",
    "    \"conv_init_mode\": \"0\",\n",
    "    \"down_up_linear_init_mode\": \"0\",\n",
    "    \"norm_init_mode\": \"0\",\n",
    "\n",
    "    \"dec_cross_last_n_frames\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9056a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO figure out the proper initialization\n",
    "# model = CuboidTransformerModel(input_shape=[10, 128, 128, 4],\n",
    "#                                target_shape=[20, 128, 128, 4],\n",
    "#                                **earthformer_config)\n",
    "\n",
    "model = CuboidTransformerModel(input_shape=[1, 128, 128, 2],\n",
    "                               target_shape=[1, 128, 128, 2],\n",
    "                               **earthformer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c293ef0",
   "metadata": {},
   "source": [
    "Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0caf5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: init_global_vectors | pretrained shape: torch.Size([8, 256]) vs model shape: torch.Size([2, 256])\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.0.weight | pretrained shape: torch.Size([64, 7, 3, 3]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.0.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.1.weight | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.1.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.3.weight | pretrained shape: torch.Size([64, 64, 3, 3]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.3.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.4.weight | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.4.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.0.weight | pretrained shape: torch.Size([256, 64, 3, 3]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.0.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.1.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.1.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.3.weight | pretrained shape: torch.Size([256, 256, 3, 3]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.3.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.4.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.4.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.0.reduction.weight | pretrained shape: torch.Size([64, 256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.1.reduction.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.1.norm.weight | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.1.norm.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: torch.Size([343, 4])\n",
      "Skipping: encoder.blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: torch.Size([64, 64])\n",
      "Skipping: encoder.blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: torch.Size([343, 4])\n",
      "Skipping: encoder.blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: torch.Size([64, 64])\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.l2g_q_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.l2g_global_kv_net.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.g2l_global_q_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.g2l_k_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.g2l_v_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.g2g_global_qkv_net.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.global_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.global_proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.global_vec_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.global_vec_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: torch.Size([343, 4])\n",
      "Skipping: encoder.blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: torch.Size([64, 64])\n",
      "Skipping: encoder.blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: torch.Size([343, 4])\n",
      "Skipping: encoder.blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: torch.Size([64, 64])\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.l2g_q_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.l2g_global_kv_net.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.g2l_global_q_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.g2l_k_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.g2l_v_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.g2g_global_qkv_net.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.global_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.global_proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.global_vec_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.global_vec_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.downsample_layers.0.reduction.weight | pretrained shape: torch.Size([512, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.downsample_layers.0.norm.weight | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.downsample_layers.0.norm.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = model.state_dict()\n",
    "# Filter the keys that match in name AND shape\n",
    "compatible_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k in model_state_dict and model_state_dict[k].shape == v.shape:\n",
    "        compatible_state_dict[k] = v\n",
    "        # print(f\"Loading: {k} | with shape: {v.shape}\")\n",
    "    else:\n",
    "        val = model_state_dict.get(k, 'MISSING')\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            val = val.shape\n",
    "        print(f\"Skipping: {k} | pretrained shape: {v.shape} vs model shape: {val}\")\n",
    "\n",
    "# load_result = model.load_state_dict(state_dict, strict=False)\n",
    "# print(\"Missing keys:\")\n",
    "# print(load_result.missing_keys)\n",
    "# print(\"Unexpected keys:\")\n",
    "# print(load_result.unexpected_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b6044",
   "metadata": {},
   "source": [
    "Adapt Earthformer to classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ab29d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = S_PCHA.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthformerClassifier(nn.Module):\n",
    "    def __init__(self, earthformer_model, num_classes=n_classes):\n",
    "        super().__init__()\n",
    "        self.model = earthformer_model\n",
    "        self.pool = nn.AdaptiveAvgPool3d((1, 1, 1))  # Pool over T, H, W\n",
    "        self.classifier = nn.Linear(self.model.target_shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)  # (B, T_out, H, W, C_out)\n",
    "        x = x.permute(0, 4, 1, 2, 3)  # → [B, C_out, T_out, H, W]\n",
    "        x = self.pool(x).squeeze()    # → [B, C_out]\n",
    "        logits = self.classifier(x)   # → [B, num_classes]\n",
    "        probs = torch.sigmoid(logits) if logits.shape[1] == 1 else torch.softmax(logits, dim=1)\n",
    "        return probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32ede748",
   "metadata": {},
   "source": [
    "# Predicting Archetypes with Earthformer\n",
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37b4b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import xarray as xr\n",
    "\n",
    "# import both nc's\n",
    "stream_path = \"../../../data/deseason_smsub_sqrtcosw/lentis_stream250_JJA_2deg_101_deseason_smsub_sqrtcosw.nc\"\n",
    "dataset_stream = xr.open_dataset(stream_path)\n",
    "\n",
    "tas_path = \"../../../data/deseason_smsub_sqrtcosw/lentis_tas_JJA_2deg_101_deseason.nc\"\n",
    "dataset_tas = xr.open_dataset(tas_path)\n",
    "\n",
    "toa_path = \"../../../data/deseason_smsub_sqrtcosw/lentis_toa_JJA_2deg_101_deseason.nc\"\n",
    "dataset_toa = xr.open_dataset(toa_path)\n",
    "\n",
    "# get S_PCHA from archetypes file\n",
    "with h5py.File('../../../data/deseason_smsub_sqrtcosw/pcha_results_8a_0d.hdf5', 'r') as f: # run from mmi393 directory or gives error\n",
    "        S_PCHA = f['/S_PCHA'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c76dc13",
   "metadata": {},
   "source": [
    "Join TAS and stream function data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5f19eca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lat_values(time_sel, lon_sel, ds, varname):\n",
    "    # Use method='nearest' to get the closest lon available\n",
    "    ds_slice = ds[varname].isel(time=time_sel).sel(lon=lon_sel, method='nearest')\n",
    "\n",
    "    # Create a dict of {lat_value: rlut_value}\n",
    "    lat_var_map = {\n",
    "        float(lat): float(ds_slice.sel(lat=lat, method='nearest').values)\n",
    "        for lat in ds_slice['lat'].values\n",
    "    }\n",
    "\n",
    "    # Pretty print\n",
    "    for lat, val in lat_var_map.items():\n",
    "        print(f\"{lat:.2f}Â°: {val:.2f}\")\n",
    "\n",
    "\n",
    "def align_ds_to_target_and_combine(olr_ds, stream_ds):\n",
    "    target_lats = stream_ds['lat']\n",
    "    spacing = np.mean(np.diff(target_lats))\n",
    "\n",
    "    start = target_lats[0] - spacing\n",
    "    n_points = 17\n",
    "    new_lats = np.linspace(start, start - spacing * (n_points-1), n_points)\n",
    "    extended_target_lats = np.concatenate([new_lats[::-1], target_lats])\n",
    "    aligned_olr_ds = olr_ds.interp(lat=extended_target_lats, method='linear')\n",
    "\n",
    "    stream_ds_ext = stream_ds.interp(lat=extended_target_lats, method='linear')\n",
    "    stream_np = stream_ds_ext['stream'].squeeze('plev').values\n",
    "\n",
    "    aligned_olr_jja = aligned_olr_ds.sel(time=aligned_olr_ds['time'].dt.month.isin([6, 7, 8]))\n",
    "    olr_np = aligned_olr_jja['rlut'].values\n",
    "\n",
    "    x_np = np.stack([stream_np, olr_np], axis=-1)\n",
    "    \n",
    "    return x_np\n",
    "\n",
    "x_np = align_ds_to_target_and_combine(dataset_toa, dataset_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ee5aae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fruits = ['apple', 'blueberry', 'cherry', 'orange']\n",
    "counts = [40, 100, 30, 55]\n",
    "bar_labels = ['red', 'blue', '_red', 'orange']\n",
    "bar_colors = ['steelblue', 'teal', 'darksalmon', 'sandybrown']\n",
    "\n",
    "ax.bar(fruits, counts, label=bar_labels, color=bar_colors)\n",
    "\n",
    "ax.set_ylabel('fruit supply')\n",
    "ax.set_title('Fruit supply by kind and color')\n",
    "ax.legend(title='Fruit color')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0bd1c713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9200, 46, 170, 2)\n",
      "w:170, h:46\n",
      "torch.Size([9200, 35, 128, 2])\n",
      "top: 46\n",
      "torch.Size([9200, 128, 128, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "print(x_np.shape)\n",
    "\n",
    "def interpolate_tensor(x_tensor, target_size=(128, 128)):\n",
    "    h,w = x_tensor.shape[1:3]\n",
    "    print(f\"w:{w}, h:{h}\")\n",
    "    target_w = target_size[1]\n",
    "    scale = target_w/w\n",
    "    target_h = int(round(h*scale))\n",
    "\n",
    "    x_tensor_perm = x_tensor.permute(0, 3, 1, 2) # now T,C,H,W\n",
    "    x_tensor_resized = F.interpolate(x_tensor_perm, size=(target_h, target_w), mode='bilinear', align_corners=False)\n",
    "    x_tensor_resized = x_tensor_resized.permute(0, 2, 3, 1) # now T, H_new, W_new, C\n",
    "\n",
    "    print(x_tensor_resized.shape)\n",
    "\n",
    "    # Center the resized tensor along height\n",
    "    out = torch.full((x_tensor.shape[0], target_size[0], target_size[1], x_tensor.shape[3]), float('nan'))\n",
    "    top = (target_size[0] - target_h) // 2\n",
    "    print(f\"top: {top}\")\n",
    "    # print(f\"top: {top}\")\n",
    "\n",
    "    out[:, top:top + target_h, :target_w, :] = x_tensor_resized\n",
    "    print(out.shape)\n",
    "\n",
    "interpolate_tensor(torch.from_numpy(x_np).float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2991d908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3480835\n",
      "0.13922119\n",
      "-0.3480835\n",
      "0.13922119\n"
     ]
    }
   ],
   "source": [
    "# group indices based on whichever archetype is maximum there\n",
    "arch_indices = np.argmax(S_PCHA, axis=0)\n",
    "\n",
    "# sanity check part 1: these results should be the same in part 2\n",
    "print(dataset_tas.isel(time=123)['tas'].isel(lon=0, lat=0).values)\n",
    "print(dataset_tas.isel(time=74)['tas'].isel(lon=4, lat=8).values)\n",
    "\n",
    "# join the nc's together\n",
    "dataset_comb = dataset_stream.assign(tas=dataset_tas['tas'])\n",
    "\n",
    "# sanity check part 2\n",
    "print(dataset_comb.isel(time=123)['tas'].isel(lon=0, lat=0).values)\n",
    "print(dataset_comb.isel(time=74)['tas'].isel(lon=4, lat=8).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45647cd",
   "metadata": {},
   "source": [
    "Add labels from AA results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "addde090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2 2 3\n",
      "4 2 2 3\n",
      "4 2 2 3\n"
     ]
    }
   ],
   "source": [
    "# sanity check 1\n",
    "print(arch_indices[0], arch_indices[5], arch_indices[6], arch_indices[9119])\n",
    "\n",
    "arch_da = xr.DataArray(arch_indices, dims=\"time\", coords={\"time\": dataset_comb.time})\n",
    "# sanity check 2 \n",
    "print(arch_da.isel(time=0).values, arch_da.isel(time=5).values, arch_da.isel(time=6).values, arch_da.isel(time=9119).values)\n",
    "\n",
    "# calculate the mean for each archetype's group\n",
    "dataset_comb_labeled = dataset_comb.assign(archetype=arch_da)\n",
    "\n",
    "# sanity check 3\n",
    "print(dataset_comb_labeled.isel(time=0)['archetype'].values,\n",
    "      dataset_comb_labeled.isel(time=5)['archetype'].values,\n",
    "      dataset_comb_labeled.isel(time=6)['archetype'].values,\n",
    "      dataset_comb_labeled.isel(time=9119)['archetype'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593acae1",
   "metadata": {},
   "source": [
    "## Dataset construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4bd6bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scistor/ivm/mmi393/.conda/envs/netcdf_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04c65e",
   "metadata": {},
   "source": [
    "From xarray dataset to pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc6866d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9200, 29, 170, 2])\n"
     ]
    }
   ],
   "source": [
    "stream = dataset_comb['stream'].squeeze('plev').values  # (T, lat, lon)\n",
    "tas = dataset_comb['tas'].values                        # (T, lat, lon)\n",
    "\n",
    "# Extract and squeeze stream function\n",
    "stream = dataset_comb['stream'].squeeze('plev').values  # (T, H, W)\n",
    "tas = dataset_comb['tas'].values                        # (T, H, W)\n",
    "\n",
    "# Stack the variables along the channel axis\n",
    "x_np = np.stack([stream, tas], axis=-1)  # shape: (T, H, W, C) where C = 2\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "x_tensor = torch.from_numpy(x_np).float()\n",
    "\n",
    "print(x_tensor.shape)  # (T=9200, H=29, W=170, C=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d8b77",
   "metadata": {},
   "source": [
    "Target construction\n",
    "\n",
    "If t+7 is belongs to another year, exclude example from labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58ececb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_final shape: torch.Size([8500, 29, 170, 2])\n",
      "y_final shape: torch.Size([8500])\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106]\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "l = 7  # lead time\n",
    "time = dataset_comb['time'].values  # format: datetime64\n",
    "arch_labels = arch_da.values        # (9200,)\n",
    "\n",
    "x_all = x_tensor  # shape: (T, H, W, C)\n",
    "x_list = []\n",
    "y_list = []\n",
    "kept_time_indices = []\n",
    "\n",
    "# Makes it so that examples from different years do not get combined\n",
    "# TODO Add data from September to include last week of August?\n",
    "for t in range(len(time) - l):\n",
    "    target_time = time[t] + np.timedelta64(l, 'D')\n",
    "    if time[t + l] == target_time:\n",
    "        x_list.append(x_all[t])\n",
    "        y_list.append(arch_labels[t + l])\n",
    "        kept_time_indices.append(t)\n",
    "\n",
    "# Stack into tensors\n",
    "x_final = torch.stack(x_list)              # shape: (N, H, W, C)\n",
    "y_final = torch.tensor(y_list, dtype=torch.long)  # shape: (N,)\n",
    "\n",
    "print(f\"x_final shape: {x_final.shape}\") # approx. 8% of the dataset is cut\n",
    "print(f\"y_final shape: {y_final.shape}\")\n",
    "print(kept_time_indices[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedebd74",
   "metadata": {},
   "source": [
    "Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb05717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO split x&y into train/test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17678ec",
   "metadata": {},
   "source": [
    "## Using Earthformer\n",
    "Import pretrained Earthformer checkpoint for EarthNet2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42d45dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from earthformer.cuboid_transformer.cuboid_transformer import CuboidTransformerModel\n",
    "#from earthformer.train_cuboid_earthnet import CuboidEarthNet2021PLModule\n",
    "from earthformer.utils.utils import download\n",
    "\n",
    "save_dir = \"./experiments\"\n",
    "\n",
    "pretrained_checkpoint_url = \"https://earthformer.s3.amazonaws.com/pretrained_checkpoints/earthformer_earthnet2021.pt\"\n",
    "local_checkpoint_path = os.path.join(save_dir, \"earthformer_earthnet2021.pt\")\n",
    "download(url=pretrained_checkpoint_url, path=local_checkpoint_path)\n",
    "\n",
    "state_dict = torch.load(local_checkpoint_path, map_location=torch.device(\"cpu\"))\n",
    "#pl_module.torch_nn_module.load_state_dict(state_dict=state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f1e064",
   "metadata": {},
   "source": [
    "Initialize Earthformer model with the correct config, based on Earthnet2021 setup in Earthformer github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1cfa4638",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthformer_config = {\n",
    "    \"base_units\": 256,\n",
    "    \"block_units\": None,\n",
    "    \"scale_alpha\": 1.0,\n",
    "\n",
    "    \"enc_depth\": [1, 1],\n",
    "    \"dec_depth\": [1, 1],\n",
    "    \"enc_use_inter_ffn\": True,\n",
    "    \"dec_use_inter_ffn\": True,\n",
    "    \"dec_hierarchical_pos_embed\": False,\n",
    "\n",
    "    \"downsample\": 2,\n",
    "    \"downsample_type\": \"patch_merge\",\n",
    "    \"upsample_type\": \"upsample\",\n",
    "\n",
    "    \"num_global_vectors\": 2,\n",
    "    \"use_dec_self_global\": False,\n",
    "    \"dec_self_update_global\": True,\n",
    "    \"use_dec_cross_global\": False,\n",
    "    \"use_global_vector_ffn\": False,\n",
    "    \"use_global_self_attn\": True,\n",
    "    \"separate_global_qkv\": True,\n",
    "    \"global_dim_ratio\": 1,\n",
    "\n",
    "    \"attn_drop\": 0.1,\n",
    "    \"proj_drop\": 0.1,\n",
    "    \"ffn_drop\": 0.1,\n",
    "    \"num_heads\": 4,\n",
    "\n",
    "    \"ffn_activation\": \"gelu\",\n",
    "    \"gated_ffn\": False,\n",
    "    \"norm_layer\": \"layer_norm\",\n",
    "    \"padding_type\": \"zeros\",\n",
    "    \"pos_embed_type\": \"t+hw\",\n",
    "    \"use_relative_pos\": True,\n",
    "    \"self_attn_use_final_proj\": True,\n",
    "\n",
    "    \"checkpoint_level\": 0,\n",
    "\n",
    "    \"initial_downsample_type\": \"stack_conv\",\n",
    "    \"initial_downsample_activation\": \"leaky\",\n",
    "    \"initial_downsample_stack_conv_num_layers\": 2,\n",
    "    \"initial_downsample_stack_conv_dim_list\": [64, 256],\n",
    "    \"initial_downsample_stack_conv_downscale_list\": [2, 2],\n",
    "    \"initial_downsample_stack_conv_num_conv_list\": [2, 2],\n",
    "\n",
    "    \"attn_linear_init_mode\": \"0\",\n",
    "    \"ffn_linear_init_mode\": \"0\",\n",
    "    \"conv_init_mode\": \"0\",\n",
    "    \"down_up_linear_init_mode\": \"0\",\n",
    "    \"norm_init_mode\": \"0\",\n",
    "\n",
    "    \"dec_cross_last_n_frames\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9056a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO figure out the proper initialization\n",
    "# model = CuboidTransformerModel(input_shape=[10, 128, 128, 4],\n",
    "#                                target_shape=[20, 128, 128, 4],\n",
    "#                                **earthformer_config)\n",
    "\n",
    "model = CuboidTransformerModel(input_shape=[1, 128, 128, 2],\n",
    "                               target_shape=[1, 128, 128, 2],\n",
    "                               **earthformer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c293ef0",
   "metadata": {},
   "source": [
    "Load pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0caf5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: init_global_vectors | pretrained shape: torch.Size([8, 256]) vs model shape: torch.Size([2, 256])\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.0.weight | pretrained shape: torch.Size([64, 7, 3, 3]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.0.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.1.weight | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.1.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.3.weight | pretrained shape: torch.Size([64, 64, 3, 3]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.3.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.4.weight | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.0.4.bias | pretrained shape: torch.Size([64]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.0.weight | pretrained shape: torch.Size([256, 64, 3, 3]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.0.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.1.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.1.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.3.weight | pretrained shape: torch.Size([256, 256, 3, 3]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.3.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.4.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.conv_block_list.1.4.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.0.reduction.weight | pretrained shape: torch.Size([64, 256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.1.reduction.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.1.norm.weight | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: initial_aux_encoder.patch_merge_list.1.norm.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: torch.Size([343, 4])\n",
      "Skipping: encoder.blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: torch.Size([64, 64])\n",
      "Skipping: encoder.blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: torch.Size([343, 4])\n",
      "Skipping: encoder.blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: torch.Size([64, 64])\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.l2g_q_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.l2g_global_kv_net.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.g2l_global_q_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.g2l_k_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.g2l_v_net.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.g2g_global_qkv_net.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.global_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.global_proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.global_vec_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.0.0.attn_l.2.global_vec_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: torch.Size([343, 4])\n",
      "Skipping: encoder.blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: torch.Size([64, 64])\n",
      "Skipping: encoder.blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: torch.Size([343, 4])\n",
      "Skipping: encoder.blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: torch.Size([64, 64])\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.l2g_q_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.l2g_global_kv_net.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.g2l_global_q_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.g2l_k_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.g2l_v_net.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.g2g_global_qkv_net.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.global_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.global_proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.global_vec_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: encoder.blocks.1.0.attn_l.2.global_vec_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.downsample_layers.0.reduction.weight | pretrained shape: torch.Size([512, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.downsample_layers.0.norm.weight | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.downsample_layers.0.norm.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.1.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.1.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_self_blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.1.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([63, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([32, 32]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([768, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.0.0.attn_l.2.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.1.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.ffn_l.2.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([19, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([10, 10]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.1.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.relative_position_bias_table | pretrained shape: torch.Size([31, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.relative_position_index | pretrained shape: torch.Size([16, 16]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.qkv.weight | pretrained shape: torch.Size([1536, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_self_blocks.1.0.attn_l.2.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.up_cross_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([1024, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([256, 1024]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([512, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.proj.weight | pretrained shape: torch.Size([256, 256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.proj.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.norm.weight | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.0.0.attn_l.0.norm.bias | pretrained shape: torch.Size([256]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_1.weight | pretrained shape: torch.Size([2048, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_1.bias | pretrained shape: torch.Size([2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_2.weight | pretrained shape: torch.Size([512, 2048]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.ffn_2.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.layer_norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.ffn_l.0.layer_norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.relative_position_bias_table | pretrained shape: torch.Size([59, 4]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.relative_position_index | pretrained shape: torch.Size([30, 30]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.q_proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.kv_proj.weight | pretrained shape: torch.Size([1024, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.proj.weight | pretrained shape: torch.Size([512, 512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.proj.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.norm.weight | pretrained shape: torch.Size([512]) vs model shape: MISSING\n",
      "Skipping: decoder.down_cross_blocks.1.0.attn_l.0.norm.bias | pretrained shape: torch.Size([512]) vs model shape: MISSING\n"
     ]
    }
   ],
   "source": [
    "model_state_dict = model.state_dict()\n",
    "# Filter the keys that match in name AND shape\n",
    "compatible_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k in model_state_dict and model_state_dict[k].shape == v.shape:\n",
    "        compatible_state_dict[k] = v\n",
    "        # print(f\"Loading: {k} | with shape: {v.shape}\")\n",
    "    else:\n",
    "        val = model_state_dict.get(k, 'MISSING')\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            val = val.shape\n",
    "        print(f\"Skipping: {k} | pretrained shape: {v.shape} vs model shape: {val}\")\n",
    "\n",
    "# load_result = model.load_state_dict(state_dict, strict=False)\n",
    "# print(\"Missing keys:\")\n",
    "# print(load_result.missing_keys)\n",
    "# print(\"Unexpected keys:\")\n",
    "# print(load_result.unexpected_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633b6044",
   "metadata": {},
   "source": [
    "Adapt Earthformer to classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ab29d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = S_PCHA.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb9109",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthformerClassifier(nn.Module):\n",
    "    def __init__(self, earthformer_model, num_classes=n_classes):\n",
    "        super().__init__()\n",
    "        self.model = earthformer_model\n",
    "        self.pool = nn.AdaptiveAvgPool3d((1, 1, 1))  # Pool over T, H, W\n",
    "        self.classifier = nn.Linear(self.model.target_shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)  # (B, T_out, H, W, C_out)\n",
    "        x = x.permute(0, 4, 1, 2, 3)  # â [B, C_out, T_out, H, W]\n",
    "        x = self.pool(x).squeeze()    # â [B, C_out]\n",
    "        logits = self.classifier(x)   # â [B, num_classes]\n",
    "        probs = torch.sigmoid(logits) if logits.shape[1] == 1 else torch.softmax(logits, dim=1)\n",
    "        return probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

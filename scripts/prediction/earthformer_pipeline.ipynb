{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9f8d68",
   "metadata": {},
   "source": [
    "# 1. Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dfddcc",
   "metadata": {},
   "source": [
    "This notebook trains a classifier to recognize spatiotemporal weather archetypes using Earthformer, a pretrained geospatial transformer. The pipeline includes data preprocessing, model adaptation, transfer learning, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b0930",
   "metadata": {},
   "source": [
    "# 2. Environment & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8909cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered based on first usage in notebook\n",
    "import numpy as np\n",
    "import h5py\n",
    "import xarray as xr\n",
    "import torch.nn.functional as F\n",
    "from datetime import timedelta\n",
    "import torch\n",
    "import os\n",
    "from earthformer.cuboid_transformer.cuboid_transformer import CuboidTransformerModel\n",
    "from earthformer.utils.utils import download\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a69e2a",
   "metadata": {},
   "source": [
    "# 3. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7fee29",
   "metadata": {},
   "source": [
    "## 3.1 Load raw input data & interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c950317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import both nc's\n",
    "stream_path = \"../../data/deseason_smsub/lentis_stream250_JJA_2deg_101_deseason_spatialsub.nc\"\n",
    "dataset_stream = xr.open_dataset(stream_path)\n",
    "\n",
    "tas_path = \"../../data/deseason_smsub/lentis_tas_JJA_2deg_101_deseason.nc\"\n",
    "dataset_tas = xr.open_dataset(tas_path)\n",
    "\n",
    "# get S_PCHA from archetypes file\n",
    "with h5py.File('../../data/deseason_smsub/pcha_results_8a.hdf5', 'r') as f: # run from mmi393 directory or gives error\n",
    "        S_PCHA = f['/S_PCHA'][:]\n",
    "\n",
    "# group indices based on whichever archetype is maximum there\n",
    "arch_indices = np.argmax(S_PCHA, axis=0)\n",
    "\n",
    "# join the nc's together\n",
    "dataset_comb = dataset_stream.assign(tas=dataset_tas['tas'])\n",
    "\n",
    "# add labels from archetypes into the dataset\n",
    "arch_da = xr.DataArray(arch_indices, dims=\"time\", coords={\"time\": dataset_comb.time})\n",
    "dataset_comb_labeled = dataset_comb.assign(archetype=arch_da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9dbc60",
   "metadata": {},
   "source": [
    "Load xarray data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9e2c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = dataset_comb['stream'].squeeze('plev').values  # (T, lat, lon)\n",
    "tas = dataset_comb['tas'].values                        # (T, lat, lon)\n",
    "\n",
    "# Extract and squeeze stream function\n",
    "stream = dataset_comb['stream'].squeeze('plev').values  # (T, H, W)\n",
    "tas = dataset_comb['tas'].values                        # (T, H, W)\n",
    "\n",
    "# Stack the variables along the channel axis\n",
    "x_np = np.stack([stream, tas], axis=-1)  # shape: (T, H, W, C) where C = 2\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "x_tensor = torch.from_numpy(x_np).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d630dcd",
   "metadata": {},
   "source": [
    "Interpolate from original (H, W) shape to (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62e410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change shape from (T, H, W, C) â†’ (T, C, H, W) for interp\n",
    "x_tensor_perm = x_tensor.permute(0, 3, 1, 2)\n",
    "\n",
    "# Resize spatial dimensions to 128x128\n",
    "x_tensor_resized = F.interpolate(x_tensor_perm, size=(128, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "# Restore dimension ordering\n",
    "x_tensor_resized = x_tensor_resized.permute(0, 2, 3, 1)\n",
    "\n",
    "# Check final shape\n",
    "print(\"Final resized shape:\", x_tensor_resized.shape) # (T, 128, 128, C)\n",
    "\n",
    "x_tensor = x_tensor_resized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efbd07d",
   "metadata": {},
   "source": [
    "## 3.2 Construct target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888b143e",
   "metadata": {},
   "source": [
    "NB: Some of the data will be cut during target construction due to the lead time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341733fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 7  # lead time\n",
    "time = dataset_comb['time'].values  # format: datetime64\n",
    "arch_labels = arch_da.values        # (9200,)\n",
    "\n",
    "x_all = x_tensor  # shape: (T, H, W, C)\n",
    "x_list = []\n",
    "y_list = []\n",
    "kept_time_indices = []\n",
    "\n",
    "# Makes it so that examples from different years do not get combined\n",
    "# TODO Add data from September to include last week of August?\n",
    "for t in range(len(time) - l):\n",
    "    target_time = time[t] + np.timedelta64(l, 'D')\n",
    "    if time[t + l] == target_time:\n",
    "        x_list.append(x_all[t])\n",
    "        y_list.append(arch_labels[t + l])\n",
    "        kept_time_indices.append(t)\n",
    "\n",
    "# Stack into tensors\n",
    "x_final = torch.stack(x_list)              # shape: (N, H, W, C)\n",
    "# TODO change y into one-hot vector encoding?\n",
    "y_final = torch.tensor(y_list, dtype=torch.long)  # shape: (N,)\n",
    "\n",
    "print(f\"x_final shape: {x_final.shape}\") # approx. 8% of the dataset is cut\n",
    "print(f\"y_final shape: {y_final.shape}\")\n",
    "print(kept_time_indices[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf8050d",
   "metadata": {},
   "source": [
    "## 3.3 Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67814e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "data_length = x_final.shape[0]\n",
    "x_train, x_test = x_final[:floor(data_length*split)], x_final[floor(data_length*split):]\n",
    "y_train, y_test = y_final[:floor(data_length*split)], y_final[floor(data_length*split):]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ff1091",
   "metadata": {},
   "source": [
    "# 4. Earthformer Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b04a2",
   "metadata": {},
   "source": [
    "## 4.1 Load Earthformer model config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55aca583",
   "metadata": {},
   "source": [
    "Load state dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c45c66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"./experiments\"\n",
    "\n",
    "pretrained_checkpoint_url = \"https://earthformer.s3.amazonaws.com/pretrained_checkpoints/earthformer_earthnet2021.pt\"\n",
    "local_checkpoint_path = os.path.join(save_dir, \"earthformer_earthnet2021.pt\")\n",
    "download(url=pretrained_checkpoint_url, path=local_checkpoint_path)\n",
    "\n",
    "state_dict = torch.load(local_checkpoint_path, map_location=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67744792",
   "metadata": {},
   "source": [
    "Load EarthNet2021 config, sourced from [Earthnet repository](https://github.com/amazon-science/earth-forecasting-transformer/blob/main/scripts/cuboid_transformer/earthnet_w_meso/earthformer_earthnet_v1.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "461c7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "earthformer_config = {\n",
    "    \"base_units\": 256,\n",
    "    \"block_units\": None,\n",
    "    \"scale_alpha\": 1.0,\n",
    "\n",
    "    \"enc_depth\": [1, 1],\n",
    "    \"dec_depth\": [1, 1],\n",
    "    \"enc_use_inter_ffn\": True,\n",
    "    \"dec_use_inter_ffn\": True,\n",
    "    \"dec_hierarchical_pos_embed\": False,\n",
    "\n",
    "    \"downsample\": 2,\n",
    "    \"downsample_type\": \"patch_merge\",\n",
    "    \"upsample_type\": \"upsample\",\n",
    "\n",
    "    \"num_global_vectors\": 2,\n",
    "    \"use_dec_self_global\": False,\n",
    "    \"dec_self_update_global\": True,\n",
    "    \"use_dec_cross_global\": False,\n",
    "    \"use_global_vector_ffn\": False,\n",
    "    \"use_global_self_attn\": True,\n",
    "    \"separate_global_qkv\": True,\n",
    "    \"global_dim_ratio\": 1,\n",
    "\n",
    "    \"attn_drop\": 0.1,\n",
    "    \"proj_drop\": 0.1,\n",
    "    \"ffn_drop\": 0.1,\n",
    "    \"num_heads\": 4,\n",
    "\n",
    "    \"ffn_activation\": \"gelu\",\n",
    "    \"gated_ffn\": False,\n",
    "    \"norm_layer\": \"layer_norm\",\n",
    "    \"padding_type\": \"zeros\",\n",
    "    \"pos_embed_type\": \"t+hw\",\n",
    "    \"use_relative_pos\": True,\n",
    "    \"self_attn_use_final_proj\": True,\n",
    "\n",
    "    \"checkpoint_level\": 0,\n",
    "\n",
    "    \"initial_downsample_type\": \"stack_conv\",\n",
    "    \"initial_downsample_activation\": \"leaky\",\n",
    "    \"initial_downsample_stack_conv_num_layers\": 2,\n",
    "    \"initial_downsample_stack_conv_dim_list\": [64, 256],\n",
    "    \"initial_downsample_stack_conv_downscale_list\": [2, 2],\n",
    "    \"initial_downsample_stack_conv_num_conv_list\": [2, 2],\n",
    "\n",
    "    \"attn_linear_init_mode\": \"0\",\n",
    "    \"ffn_linear_init_mode\": \"0\",\n",
    "    \"conv_init_mode\": \"0\",\n",
    "    \"down_up_linear_init_mode\": \"0\",\n",
    "    \"norm_init_mode\": \"0\",\n",
    "\n",
    "    \"dec_cross_last_n_frames\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088a39f8",
   "metadata": {},
   "source": [
    "## 4.2 Initialize model & load pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e5a1ab",
   "metadata": {},
   "source": [
    "Initialize Earthformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d79d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "EFmodel = CuboidTransformerModel(input_shape=[1, 128, 128, 2],\n",
    "                               target_shape=[1, 128, 128, 2],\n",
    "                               **earthformer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92898d72",
   "metadata": {},
   "source": [
    "Filter and log matching pretrained weights from state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0fd415",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = EFmodel.state_dict()\n",
    "# Filter the keys that match in name AND shape\n",
    "compatible_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    if k in model_state_dict and model_state_dict[k].shape == v.shape:\n",
    "        compatible_state_dict[k] = v\n",
    "        print(f\"Loading: {k} | with shape: {v.shape}\")\n",
    "    else:\n",
    "        val = model_state_dict.get(k, 'MISSING')\n",
    "        if isinstance(val, torch.Tensor):\n",
    "            val = val.shape\n",
    "        print(f\"Skipping: {k} | pretrained shape: {v.shape} vs model shape: {val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0494ed",
   "metadata": {},
   "source": [
    "Load compatible keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6b560",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_result = EFmodel.load_state_dict(state_dict, strict=False)\n",
    "print(\"Missing keys:\")\n",
    "print(load_result.missing_keys)\n",
    "print(\"Unexpected keys:\")\n",
    "print(load_result.unexpected_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47455bbb",
   "metadata": {},
   "source": [
    "# 5. Classifier Head Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27259f1d",
   "metadata": {},
   "source": [
    "## 5.1 Wrap Earthformer into classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c74a311",
   "metadata": {},
   "source": [
    "Define classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a595a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarthformerClassifier(nn.Module):\n",
    "    def __init__(self, earthformer_model, num_classes=8):\n",
    "        super().__init__()\n",
    "        self.model = earthformer_model\n",
    "        self.pool = nn.AdaptiveAvgPool3d((1, 1, 1))  # Pool over T, H, W\n",
    "        self.classifier = nn.Linear(self.model.target_shape[-1], num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)  # (B, T_out, H, W, C_out)\n",
    "        x = x.permute(0, 4, 1, 2, 3)  # â†’ [B, C_out, T_out, H, W]\n",
    "        x = self.pool(x).squeeze()    # â†’ [B, C_out]\n",
    "        logits = self.classifier(x)   # â†’ [B, num_classes]\n",
    "        probs = torch.sigmoid(logits) if logits.shape[1] == 1 else torch.softmax(logits, dim=1)\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c26511",
   "metadata": {},
   "source": [
    "Instantiate classifier with EF model from previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e700f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 8\n",
    "EFClassifier = EarthformerClassifier(EFmodel, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e81c8cb",
   "metadata": {},
   "source": [
    "## 5.2 Freeze pretrained layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67262e6",
   "metadata": {},
   "source": [
    "Freeze everything except classifier head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881014f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in EFmodel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in EFClassifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f1475b",
   "metadata": {},
   "source": [
    "Freeze encoder/decoder blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24942c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in EFModel.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in EFModel.decoder.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345874e9",
   "metadata": {},
   "source": [
    "# 6. Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71df4f6",
   "metadata": {},
   "source": [
    "## 6.1 Define loss & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727ebbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Entropy Loss for classification\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "lr = 5e-4  # needs to be adjusted if finetuning\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5166af8",
   "metadata": {},
   "source": [
    "## 6.2 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aecb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(model, batch_x, batch_y):\n",
    "    model.train()\n",
    "    logits = model(batch_x)\n",
    "    loss = loss_func(logits, batch_y)\n",
    "    return loss\n",
    "\n",
    "def validation_step(model, batch_x, batch_y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(batch_x)\n",
    "        loss = loss_func(logits, batch_y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == batch_y).float().mean()\n",
    "    return loss.item(), acc.item()\n",
    "\n",
    "def compute_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            logits = model(x)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            total_correct += (preds == y).sum().item()\n",
    "            total += y.size(0)\n",
    "    return total_correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f3395c",
   "metadata": {},
   "source": [
    "# 7. Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8729049",
   "metadata": {},
   "source": [
    "Train the classifier over an appropriate number of epochs, log training and validation loss (and accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765a979e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "train_losses, val_losses, val_accuracies = [], [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        loss = training_step(EFClassifier, x_batch, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    train_losses.append(epoch_loss / len(train_loader))\n",
    "\n",
    "    val_loss, val_acc = 0.0, 0.0\n",
    "    for x_batch, y_batch in val_loader:\n",
    "        batch_loss, batch_acc = validation_step(EFClassifier, x_batch, y_batch)\n",
    "        val_loss += batch_loss\n",
    "        val_acc += batch_acc\n",
    "    \n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    val_accuracies.append(val_acc / len(val_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_losses[-1]:.4f} | Val Loss: {val_losses[-1]:.4f} | Val Acc: {val_accuracies[-1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a18189",
   "metadata": {},
   "source": [
    "# 8. Results & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "697e2a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot loss/accuracy vs. epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672e70ac",
   "metadata": {},
   "source": [
    "# 9. Save Model & Export Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcc1be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "output_dir = f\"outputs/{timestamp}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save model and optimizer states\n",
    "torch.save({\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "}, os.path.join(output_dir, \"checkpoint.pt\"))\n",
    "\n",
    "# Save loss history\n",
    "with open(os.path.join(output_dir, \"train_loss.json\"), \"w\") as f:\n",
    "    json.dump(train_loss_history, f)\n",
    "with open(os.path.join(output_dir, \"val_loss.json\"), \"w\") as f:\n",
    "    json.dump(val_loss_history, f)\n",
    "\n",
    "# Plot and save loss curves\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_loss_history, label='Train Loss')\n",
    "plt.plot(val_loss_history, label='Val Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(output_dir, \"loss_curve.png\"))\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
